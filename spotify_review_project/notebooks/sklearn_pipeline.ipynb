{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit Learn pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting to ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import pandas as pd\n",
    "\n",
    "## Transformers\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "## Classifiers\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "## Model selection and Pipeline utils\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "## Text processing libraries\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import emoji\n",
    "import contractions ## from here: https://github.com/kootenpv/contractions\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data\n",
    "1. Drop duplicate reviews\n",
    "2. Drop unused columns\n",
    "3. Create column for review length\n",
    "4. Drop unusually long reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read data and copy\n",
    "spotify = pd.read_csv(\"../data/raw/spotify_review_kaggle.csv\")\n",
    "data_in = spotify.copy()\n",
    "data_in = data_in.drop_duplicates(subset=\"Review\")\n",
    "data_in.drop([\"Time_submitted\", \"Total_thumbsup\", \"Reply\"], axis=1, inplace=True)\n",
    "data_in[\"Length\"] = data_in[\"Review\"].str.split(\" \").str.len()\n",
    "data_in = data_in[data_in.Length < 150]\n",
    "data = data_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Sentiment\n",
    "* Ratings 1, 2 and 3 as _negative_\n",
    "* Ratings 4 and 5 as _positive_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code sentiment from rating (1 or 2 == bad, 3 == neutral, 4 or 5 == good)\n",
    "def get_sentiment(rating):\n",
    "    if rating == 1 or rating == 2:\n",
    "        return \"negative\"\n",
    "    # if rating == 3:\n",
    "    #     return \"neutral\"\n",
    "    if rating == 3 or rating == 4 or rating == 5:\n",
    "        return \"positive\"\n",
    "\n",
    "data[\"Sentiment\"] = data[\"Rating\"].apply(get_sentiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Stopwords\n",
    "Here we grab the NLTK stopwords but want to keep \"not\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_stop_words = set(stopwords.words('english'))\n",
    "our_stop_words.remove(\"not\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate emojis to text\n",
    "We use this function here to translate emojis to text. We will need this function later on to include in our prediction pipeline, so that new data will be transformed in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define a function to translate emojis text\n",
    "def translate_emoji(sentence):\n",
    "  return emoji.demojize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Review\"] = data[\"Review\"].apply(translate_emoji)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change contractions like _I'm_, _they're_ etc. to _I am_, _they are_ etc. We'll use this again later in our prediction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_contractions(sentence):\n",
    "    return contractions.fix(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Review\"] = data[\"Review\"].apply(translate_contractions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove strange fonts\n",
    "There are some strange unreadable fonts like 𝚝𝚑𝚒𝚜 in the reviews. Here we remove them. Again, we'll use this function later in our make_prediction function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_strange_fonts(sentence):\n",
    "    return re.sub(r'[^\\x00-\\x7f]', r'', sentence)\n",
    "\n",
    "\n",
    "data['Review'] = data['Review'].apply(lambda sentence: list(map(remove_strange_fonts, sentence)))\n",
    "data[\"Review\"] = data[\"Review\"].apply(lambda sentence: \"\".join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Great music service, the audio is high quality...\n",
       "1        Please ignore previous negative rating. This a...\n",
       "2        This pop-up \"Get the best Spotify experience o...\n",
       "3          Really buggy and terrible to use as of recently\n",
       "4        Dear Spotify why do I get songs that I did not...\n",
       "                               ...                        \n",
       "61589    Even though it was communicated that lyrics fe...\n",
       "61590    Use to be sooo good back when I had it, and wh...\n",
       "61591    This app would be good if not for it taking ov...\n",
       "61592    The app is good hard to navigate and will not ...\n",
       "61593    Its good but sometimes it does not load the mu...\n",
       "Name: Review, Length: 61346, dtype: object"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Review\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create features and target and split into training, development and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X and y:  (61346,) (61346,)\n"
     ]
    }
   ],
   "source": [
    "X = data[\"Review\"]\n",
    "y = data[\"Sentiment\"]\n",
    "print(\"Shape of X and y: \", X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36807,) (24539,) (36807,) (24539,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, stratify = y, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12269,) (12270,) (12269,) (12270,)\n"
     ]
    }
   ],
   "source": [
    "X_dev, X_test, y_dev, y_test = train_test_split(X_test, y_test, test_size=0.5, stratify = y_test, random_state=42)\n",
    "print(X_dev.shape, X_test.shape, y_dev.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pipeline for hyperparameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define parameters for the pipelines\n",
    "\n",
    "## Parameters for Feature extraction\n",
    "transformer_parameters = {\n",
    "    'vect__max_features': (None, 5000, 10000),\n",
    "    \"vect__ngram_range\": ((1, 1), (1, 2), (1,3)),  # unigrams or bigrams\n",
    "    'tfidf__norm': ('l1','l2')\n",
    "}\n",
    "\n",
    "## Parameters for classifiers\n",
    "logistic_regression_parameters = {\n",
    "        'clf': (LogisticRegression(\n",
    "            fit_intercept=True,\n",
    "            class_weight=None,\n",
    "            solver=\"lbfgs\",\n",
    "            max_iter=1000,\n",
    "            random_state=42),\n",
    "            ),\n",
    "        'clf__penalty': (\"none\",\"l1\",\"l2\",\"elasticnet\"),\n",
    "        'clf__C': (0.01, 0.1, 1, 10, 100),\n",
    "        \"clf__l1_ratio\": (0.01, 0.1,1)\n",
    "    }\n",
    "\n",
    "svc_parameters = {\n",
    "        'clf': (SVC(\n",
    "            probability=True,\n",
    "            tol=1e-3,\n",
    "            class_weight=None,\n",
    "            max_iter=1000,\n",
    "            random_state=42),\n",
    "            ),\n",
    "        'clf__kernel': (\"linear\",\"poly\",\"rbf\",\"sigmoid\"),\n",
    "        'clf__C': (0.001, 0.01, 0.1, 1, 10, 100),\n",
    "        \"clf__gamma\": (0.001, 0.01, 0.1, 1, 10, 100, \"scale\"),\n",
    "        \"clf__shrinking\": (True, False),\n",
    "        \"clf__decision_function_shape\": (\"ovr\", \"ovo\")\n",
    "    }\n",
    "\n",
    "rf_parameters = {\n",
    "        'clf': (RandomForestClassifier(\n",
    "            criterion=\"gini\",\n",
    "            min_samples_leaf=5,\n",
    "            oob_score=True,\n",
    "            class_weight=None,\n",
    "            random_state=42),\n",
    "            ),\n",
    "        'clf__n_estimators': (10, 100, 1000),\n",
    "        'clf__max_depth': (3, 5, 10, 50, 100, None),\n",
    "        #\"clf__min_samples_split\": (2, 5, 10),\n",
    "        #\"clf__max_features\": (\"sqrt\", \"log2\"),\n",
    "        #\"clf__bootstrap\": (True, False)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Grid Search Params\n",
    "logistic_parameters = [\n",
    "    transformer_parameters,\n",
    "    logistic_regression_parameters\n",
    "]\n",
    "\n",
    "svc_parameters = [\n",
    "    transformer_parameters,\n",
    "    svc_parameters\n",
    "]\n",
    "\n",
    "random_forest_parameters = [\n",
    "    transformer_parameters,\n",
    "    rf_parameters\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting  LogisticRegression Classifier\n",
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters:\n",
      "[{'tfidf__norm': ('l1', 'l2'),\n",
      "  'vect__max_features': (None, 5000, 10000),\n",
      "  'vect__ngram_range': ((1, 1), (1, 2), (1, 3))},\n",
      " {'clf': (LogisticRegression(C=0.1, max_iter=1000, random_state=42),),\n",
      "  'clf__l1_ratio': (0.01, 0.1, 1),\n",
      "  'clf__penalty': ('none', 'l1', 'l2', 'elasticnet')}]\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Best score: 0.835\n",
      "Best parameters set:\n",
      "\ttfidf__norm: 'l2'\n",
      "\tvect__max_features: 10000\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\tclf: LogisticRegression()\n",
      "\tclf__l1_ratio: None\n",
      "\tclf__penalty: 'l2'\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "all_parameter_sets = [logistic_parameters]\n",
    "classifiers = [LogisticRegression()]\n",
    "\n",
    "for parameter_sets, classifier in zip(all_parameter_sets, classifiers):\n",
    "    #define pipeline\n",
    "    pipeline = Pipeline(\n",
    "        [   \n",
    "            (\"vect\", CountVectorizer(stop_words=our_stop_words)),\n",
    "            (\"tfidf\", TfidfTransformer()),\n",
    "            (\"clf\", classifier)\n",
    "        ]\n",
    "    )\n",
    "    ## Perform grid search CV\n",
    "    print(\"Fitting \", str(classifier)[:-2]+\" Classifier\")\n",
    "    gs = GridSearchCV(pipeline, parameter_sets, n_jobs=-1, verbose=1, scoring=\"accuracy\", cv = 3) ## -1 means all processors\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameter_sets)\n",
    "\n",
    "    gs.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Best score: %0.3f\" % gs.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "\n",
    "    best_parameters = gs.best_estimator_.get_params()\n",
    "\n",
    "    for parameter_set in parameter_sets:\n",
    "        for param_name in sorted(parameter_set.keys()):\n",
    "            print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "    print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_classifier =  gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = our_classifier.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.874\n",
      "Accuracy on development set: 0.836\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on training set: %0.3f\" % our_classifier.score(X_train, y_train))\n",
    "print(\"Accuracy on development set: %0.3f\" % our_classifier.score(X_dev, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6201, 1126],\n",
       "       [ 885, 4057]], dtype=int64)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_dev, predictions, labels=[\"positive\", \"negative\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.88      0.85      0.86      7327\n",
      "    negative       0.78      0.82      0.80      4942\n",
      "\n",
      "    accuracy                           0.84     12269\n",
      "   macro avg       0.83      0.83      0.83     12269\n",
      "weighted avg       0.84      0.84      0.84     12269\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_dev, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(classifier, sentence):\n",
    "    temp = translate_emoji(sentence)\n",
    "    temp = translate_contractions(temp)\n",
    "    temp = remove_strange_fonts(temp)\n",
    "    return classifier.predict([temp])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive'], dtype=object)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = \"I ❤️ Spotify\"\n",
    "\n",
    "#review = \"Well, it is Spotify, what can one say, never works as it should\"\n",
    "\n",
    "make_prediction(our_classifier, review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative'], dtype=object)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = \"I hate Spotify\"\n",
    "\n",
    "make_prediction(our_classifier, review)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba89649856ee0ebb4f5d432fb84526bc860cc927389973834ceef247cbb7120d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
